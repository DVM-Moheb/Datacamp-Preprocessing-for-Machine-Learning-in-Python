{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b73971d-564c-4c28-aebf-170cb33344d4",
   "metadata": {},
   "source": [
    "# Chapter #2: Standardizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce9e9f8-6ade-487f-ae11-3c6533c434a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b528376-a983-47e8-89f4-aa93a03c6847",
   "metadata": {},
   "source": [
    "## 1. Standardizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaadea2-62d9-4db4-81a7-16acbb32704e",
   "metadata": {},
   "source": [
    "1. Standardizing Data\n",
    "It's possible that you'll come across datasets with lots of numerical noise built in, such as lots of variance or differently-scaled data. The preprocessing solution for that is standardization.\n",
    "\n",
    "2. What is standardization?\n",
    "Standardization is a preprocessing method used to transform continuous data to make it look normally distributed. In scikit-learn, this is often a necessary step, because many models assume that the data you are training on is normally distributed, and if it isn't, you risk biasing your model. You can standardize your data in different ways, but in this course we're going to talk about two methods: log normalization and scaling. It's also important to note that standardization is a preprocessing method applied to continuous, numerical data. You'll learn methods for dealing with categorical data later in the course.\n",
    "\n",
    "3. When to standardize: models\n",
    "There are a few different scenarios in which you want to standardize your data. First, if you're working with any kind of model that uses a linear distance metric or operates in a linear space like k-nearest neighbors, linear regression, or k-means clustering, the model is assuming that the data and features you're giving it are related in a linear fashion, or can be measured with a linear distance metric. There are a number of models that deal with nonlinear spaces, but for those models that are in a linear space, the data must also be in that space. The case when a feature or features in your dataset have high variance is related to this. This could bias a model that assumes the data is normally distributed. If a feature in your dataset has a variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset. Modeling a dataset that contains continuous features that are on different scales is another scenario to watch out for. For example, consider a dataset that contains a column related to height and another related to weight. In order to compare these features, they must be in the same linear space, and therefore must be standardized in some way. All of these scenarios assume you're working with a model that makes some kind of linearity assumptions, however. There are a number of models that are perfectly fine operating in a nonlinear space or do a certain amount of standardization upon input, but that's outside the scope of this course.\n",
    "\n",
    "4. Let's practice!\n",
    "Now that you've learned when to standardize your data, let's test your knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a9cda-1e62-4f88-83b5-0b64ffc816cf",
   "metadata": {},
   "source": [
    "### 1.1. When to standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f84d37-d684-43d1-b30c-6614d77b7b1f",
   "metadata": {},
   "source": [
    "Now that you've learned when it is appropriate to standardize your data, which of these scenarios would you NOT want to standardize?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51322e83-bd9e-4d01-b62f-e9a5cacd0910",
   "metadata": {},
   "source": [
    "Possible Answers:\n",
    "- A column you want to use for modeling has extremely high variance.\n",
    "- You have a dataset with several continuous columns on different scales and you'd like to use a linear model to train the data.\n",
    "- The models you're working with use some sort of distance metric in a linear space, like the Euclidean metric.\n",
    "- Your dataset is comprised of categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37097641-2b98-4fe8-a536-a9184a7f158b",
   "metadata": {},
   "source": [
    "> Your dataset is comprised of categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0aeedd-7a40-4935-b711-2436bed32640",
   "metadata": {},
   "source": [
    "### 1.2. Modeling without normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e89fb-afc7-467c-a84b-ec3ec98b4654",
   "metadata": {},
   "source": [
    "Let's take a look at what might happen to your model's accuracy if you try to model data without doing some sort of standardization first. Here we have a subset of the `wine` dataset. One of the columns, `Proline`, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which you'll learn about in the next section.\n",
    "\n",
    "The scikit-learn model training process should be familiar to you at this point, so we won't go too in-depth with it. You already have a k-nearest neighbors model available (`knn`) as well as the `X` and `y` sets you need to fit and score on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b716dc-8e83-4190-bb38-8e2bb807cae4",
   "metadata": {},
   "source": [
    "- Getting everything ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f1641b-121b-40bb-821b-30ab07584f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data:\n",
    "wine = pd.read_csv(\"./data/wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ec43d4-f439-42e8-af4d-fa9bc7692c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the shape:\n",
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3bdc0dd-670c-4880-8c96-7fc5f8c267e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0     1    14.23        1.71  2.43               15.6        127   \n",
       "1     1    13.20        1.78  2.14               11.2        100   \n",
       "2     1    13.16        2.36  2.67               18.6        101   \n",
       "3     1    14.37        1.95  2.50               16.8        113   \n",
       "4     1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the first 5 rows:\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "865cff8a-f1ae-43a5-9a5c-10b7157cae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature matrix (X):\n",
    "X = wine[['Proline', 'Total phenols', 'Hue', 'Nonflavanoid phenols']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b672719a-0b18-42cc-87bf-477563faf1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the target column (y):\n",
    "y = wine['Type'].copy()\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eee0188f-cf38-415e-9d57-c11deaaa2f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model:\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc32520-f8d5-4549-abdf-11409ffcf3a3",
   "metadata": {},
   "source": [
    "- Split up the `X` and `y` sets into training and test sets using `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "727d6f1c-8575-489e-bcbc-7a44e2dd50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training & hold-out sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16abf4e4-d730-458f-ac2b-38cbf34796f0",
   "metadata": {},
   "source": [
    "- Use the `knn` model's `.fit()` method on the `X_train` data and `y_train` labels, to fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8c28a11-e054-4224-8002-d917bbdb7dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model:\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ae49e-cb2b-4a81-942a-46312668055d",
   "metadata": {},
   "source": [
    "- Print out the `knn` model's `.score()` on the `X_test` data and `y_test` labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789ad92c-da74-4b4c-b071-a3c97b5d8ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model performance using .score():\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bea654-5ba1-4cf9-88d9-91bc3fb783e2",
   "metadata": {},
   "source": [
    "## 2. Log normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f5e17-79aa-4d9f-91ba-0b6ac3c2ae33",
   "metadata": {},
   "source": [
    "1. Log normalization\n",
    "The first method we'll cover for standardization is log normalization.\n",
    "\n",
    "2. What is log normalization?\n",
    "Log normalization is a method for standardizing your data that can be useful when you have a particular column with high variance. As you saw in the previous section's exercise, training a k-nearest neighbors classifier on that subset of the wine dataset didn't get a very high accuracy score. This is because within that subset, the Proline colummn has extremely high variance, which is affecting the accuracy of the classifier. Log normalization applies a log transformation to your values, which transforms your values onto a scale that approximates normality, an assumption about your data that a lot of models make. The method of log normalization we're going to work with in Python takes the natural log of each number in the left hand column, which is simply the exponent you would raise above the mathematical constant e (approximately equal to 2.718) to get that number. So, looking at the table on the slide, the log of 30 is 3.4, because e to the power of 3.4 equals 30. Log normalization is a good strategy when you care about relative changes in a linear model, when you still want to capture the magnitude of change, and when you want to keep everything in the positive space. It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling.\n",
    "\n",
    "3. Log normalization in Python\n",
    "Applying log normalization to data in Python is fairly straightforward. We can use the log function from Numpy to do the trick. Here we have a dataframe of some values. If we check the variance of the columns, you can see that column 2 has a significantly higher variance than column 1. To apply log normalization to column 2, we need the log function from numpy. we can pass the column we want to log normalize directly into the function. If we take a look at both column 2 and the log-normalized column-2, you can see that the transformation has scaled down the values. If we check the variance of both column 1 and the log-normalized column 2, you can see that the variances are much closer together now.\n",
    "\n",
    "4. Let's practice!\n",
    "Now it's your turn! Let's take a look at the wine dataset again and do some normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f07cccb-c6ab-47d0-8195-6a6217e58420",
   "metadata": {},
   "source": [
    "### 2.1. Checking the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2acf9e9-b346-4776-9e60-f023dd08ad81",
   "metadata": {},
   "source": [
    "Check the variance of the columns in the `wine` dataset. Out of the four columns listed in the multiple choice section, which column is a candidate for normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f431505f-866b-4af3-a284-1a1c55169ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type                                0.600679\n",
       "Alcohol                             0.659062\n",
       "Malic acid                          1.248015\n",
       "Ash                                 0.075265\n",
       "Alcalinity of ash                  11.152686\n",
       "Magnesium                         203.989335\n",
       "Total phenols                       0.391690\n",
       "Flavanoids                          0.997719\n",
       "Nonflavanoid phenols                0.015489\n",
       "Proanthocyanins                     0.327595\n",
       "Color intensity                     5.374449\n",
       "Hue                                 0.052245\n",
       "OD280/OD315 of diluted wines        0.504086\n",
       "Proline                         99166.717355\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the variance of different features:\n",
    "wine.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fb8c8-4741-493e-a826-7415128517a2",
   "metadata": {},
   "source": [
    "Possible Answers:\n",
    "- Alcohol.\n",
    "- Proline.\n",
    "- Proanthocyanins.\n",
    "- Ash."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c898be8c-5bef-4726-ae47-b53fca9c4c3f",
   "metadata": {},
   "source": [
    "> Proline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35baba33-d7e3-47fb-b0d3-b1d669e2bc28",
   "metadata": {},
   "source": [
    "### 2.2. Log normalization in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766dc18-9254-4f5b-8cc6-0adfb71a4f92",
   "metadata": {},
   "source": [
    "Now that we know that the `Proline` column in our wine dataset has a large amount of variance, let's log normalize it.\n",
    "\n",
    "Numpy has been imported as `np` in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7199d-dead-46c1-b076-84933a3797de",
   "metadata": {},
   "source": [
    "- Print out the variance of the `Proline` column for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d74e3351-9115-4c14-928b-b7b986e94b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99166.71735542428"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the variance of Proline column:\n",
    "wine['Proline'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e08fce-c92b-4b1c-bbd6-65251911fa5d",
   "metadata": {},
   "source": [
    "- Use the `np.log()` function on the `Proline` column to create a new, log-normalized column named `Proline_log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faeb367c-907c-4952-b971-5083be9c2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the Proline column using np.log():\n",
    "wine['Proline_log'] = np.log(wine['Proline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ea474-2daf-4f2c-bc21-a6378fbfec13",
   "metadata": {},
   "source": [
    "- Print out the variance of the `Proline_log` column to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d28c09-7405-4a66-8177-100b9d49c25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17231366191842018"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the variance of new Proline_log column:\n",
    "wine['Proline_log'].var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9859e-c8d4-48e8-a754-2a76d5b13059",
   "metadata": {},
   "source": [
    "## 3. Scaling data for feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f66fd4-1d8f-4f37-84e3-046e566583e8",
   "metadata": {},
   "source": [
    "1. Scaling data\n",
    "Let's move on to talking about scaling our data.\n",
    "\n",
    "2. What is feature scaling?\n",
    "Scaling is a method of standardization that's most useful when you're working with a dataset that contains continuous features that are on different scales, and you're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors). Feature scaling transforms the features in your dataset so they have a mean of zero and a variance of one. This will make it easier to linearly compare features. This is a requirement for many models in scikit-learn.\n",
    "\n",
    "3. How to scale data\n",
    "Let's take a look at another dataframe. In each column, we have numbers that are relatively close within the column, but not across columns. If we look at the variance, it's relatively low across columns. To better model this data, scaling would be a good choice here.\n",
    "\n",
    "4. How to scale data\n",
    "Scikit-learn has a variety of scaling methods, but we're only going to focus on the standard scaler method, imported from preprocessing. This method works by removing the mean and scaling each feature to have unit variance. There's a simpler scale function in scikit-learn, but the benefit of using the Standard Scaler object is that you can apply the same transformation on other data, like a test set, or new data that's part of the same set, for example, without having to rescale everything. So once we have the standard scaler method, we can apply the fit transform function on the dataframe. We can reconvert the output of fit transform, which is a numpy array, to a dataframe to look at it more easily. If we take a look at the newly scaled dataframe, we can see that the values have been scaled down, and if we calculate the variance by column, it's not only close to 1, but it's now the same for all of our features.\n",
    "\n",
    "5. Let's practice!\n",
    "Now it's your turn to try scaling data in Python and scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e7ff6-7012-4a87-b640-45c610485172",
   "metadata": {},
   "source": [
    "### 3.1. Scaling data - investigating columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e98669-45ee-497c-a8a9-66f0323b47a1",
   "metadata": {},
   "source": [
    "We want to use the `Ash`, `Alcalinity of ash`, and `Magnesium` columns in the `wine` dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model. Using `describe()` to return descriptive statistics about this dataset, which of the following statements are true about the scale of data in these columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "819317cc-ed40-4ef3-8879-5c4057cb344d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ash</th>\n",
       "      <td>178.0</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.5575</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <td>178.0</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>10.60</td>\n",
       "      <td>17.20</td>\n",
       "      <td>19.50</td>\n",
       "      <td>21.5000</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Magnesium</th>\n",
       "      <td>178.0</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>70.00</td>\n",
       "      <td>88.00</td>\n",
       "      <td>98.00</td>\n",
       "      <td>107.0000</td>\n",
       "      <td>162.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count       mean        std    min    25%    50%       75%  \\\n",
       "Ash                178.0   2.366517   0.274344   1.36   2.21   2.36    2.5575   \n",
       "Alcalinity of ash  178.0  19.494944   3.339564  10.60  17.20  19.50   21.5000   \n",
       "Magnesium          178.0  99.741573  14.282484  70.00  88.00  98.00  107.0000   \n",
       "\n",
       "                      max  \n",
       "Ash                  3.23  \n",
       "Alcalinity of ash   30.00  \n",
       "Magnesium          162.00  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring some statistics:\n",
    "wine[['Ash', 'Alcalinity of ash', 'Magnesium']].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e61c0-d08d-4d1f-bdbf-b9d05feea341",
   "metadata": {},
   "source": [
    "Possible Answers:\n",
    "- The max of `Ash` is 3.23, the max of `Alcalinity of ash` is 30, and the max of `Magnesium` is 162.\n",
    "- The means of `Ash` and `Alcalinity of ash` are less than 20, while the mean of `Magnesium` is greater than 90.\n",
    "- The standard deviations of `Ash` and `Alcalinity of ash` are equal.\n",
    "- 1 and 2 are true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d2715-2f83-4481-bc7f-72668f6e8cd0",
   "metadata": {},
   "source": [
    "> 1 and 2 are true."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc759a95-3990-4e5e-8aa7-ab12d772da54",
   "metadata": {},
   "source": [
    "### 3.2. Scaling data - standardizing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b498b2-4e42-412a-b566-2f0e1564537a",
   "metadata": {},
   "source": [
    "Since we know that the `Ash`, `Alcalinity of ash`, and `Magnesium` columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7199c-c6d5-4328-a073-9c7f60485065",
   "metadata": {},
   "source": [
    "- Import `StandardScaler` from `sklearn.preprocessing`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee12cfb-a465-4ddd-97a8-ea49b0e4b251",
   "metadata": {},
   "source": [
    "> Done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e5ac69-eea6-45a4-aa4f-89210b2863ed",
   "metadata": {},
   "source": [
    "- Create the `StandardScaler()` method and store in a variable named `ss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae064dc-786d-4c82-95a2-96e4a9cc6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the scaler:\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde6670a-660c-4f67-b94c-ba3a2830d567",
   "metadata": {},
   "source": [
    "- Create a subset of the wine DataFrame of the `Ash`, `Alcalinity of ash`, and `Magnesium` columns, store in a variable named `wine_subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a30fbab0-f2be-4365-a174-84cb71442820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting the data:\n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e38fc-9787-4787-be4e-1870425cdc41",
   "metadata": {},
   "source": [
    "- Apply the `ss.fit_transform()` method to the `wine_subset` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfaa57fe-2d34-4481-b8cb-9ed32156cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data subset:\n",
    "wine_subset_scaled = pd.DataFrame(ss.fit_transform(wine_subset), columns=wine_subset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca09a401-b536-4e26-b623-4876733cc710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ash                  1.00565\n",
       "Alcalinity of ash    1.00565\n",
       "Magnesium            1.00565\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the variance of the scaled data:\n",
    "wine_subset_scaled.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ace28d-57f5-4e63-9470-47376779da9e",
   "metadata": {},
   "source": [
    "## 4. Standardized data and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d24475-14fc-4543-8cc1-0663d875d6c8",
   "metadata": {},
   "source": [
    "1. Standardized data and modeling\n",
    "Now that we've learned a couple of different methods for standardization, it's time to put this into practice with modeling. As mentioned before, many models in scikit-learn require your data to be scaled appropriately across columns, otherwise you risk biasing your results. The last part of this section will be dedicated to modeling data on both unscaled and scaled data, so you can see the difference in model performance. The model we're going to use is k-nearest neighbors.\n",
    "\n",
    "2. K-nearest neighbors\n",
    "You should already be a little familiar with both k-nearest neighbors, as well as the scikit-learn workflow, based on previous courses, but we'll do a quick review of both. K-nearest neighbors is a model that classifies data based on its distance to training set data. A new data point is assigned a label based on the class that the majority of surrounding data points belong to. The workflow for training a model in scikit-learn is pretty simple. You will want to do all of your preprocessing first, of course. Before you train your model, it's very important that you split up your data into training and test sets, to avoid overfitting. That's easily done with scikit-learn's train test split function, shown here. Once you've done that, it's just a matter of fitting the model to your training set, and then running your unseen test set through the model. Most models have a score function, which can be used to evaluate your model's performance.\n",
    "\n",
    "3. Let's practice!\n",
    "All of that should look familiar to you. If not, check out some of the other courses related to Python and machine learning. Otherwise, it's your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda040e7-0ef8-4223-98cb-3ff6f6417518",
   "metadata": {},
   "source": [
    "### 4.1. KNN on non-scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25128680-af33-48b0-abf8-7c7b7a7e4564",
   "metadata": {},
   "source": [
    "Let's first take a look at the accuracy of a K-nearest neighbors model on the `wine` dataset without standardizing the data. The `knn` model as well as the `X` and `y` data and labels sets have been created already. Most of this process of creating models in scikit-learn should look familiar to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2effbe01-9860-4b5d-ae41-6dbc0d2aea38",
   "metadata": {},
   "source": [
    "- Getting everything ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "069893b6-d1fb-435f-a959-087a86c3d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data:\n",
    "wine = pd.read_csv(\"./data/wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a338ccbf-8c96-4925-af79-54f3efe45bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the shape:\n",
    "wine.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "483ebed6-bb63-4093-b18f-16bacf7a4cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0     1    14.23        1.71  2.43               15.6        127   \n",
       "1     1    13.20        1.78  2.14               11.2        100   \n",
       "2     1    13.16        2.36  2.67               18.6        101   \n",
       "3     1    14.37        1.95  2.50               16.8        113   \n",
       "4     1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the first 5 rows:\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d1881f2-9250-47a3-b7cf-b7983a54c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature matrix (X):\n",
    "X = wine.drop(columns='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25a07ff9-5879-41d8-bab7-09d28b75d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the target column (y):\n",
    "y = wine['Type'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56838e0b-d4d2-4bed-83bf-7831338c24e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model:\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50528751-6b75-429c-8b6e-c56aef724db8",
   "metadata": {},
   "source": [
    "- Split the dataset into training and test sets using `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bf535c0-86a2-4069-8c48-1058c93c91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training & hold-out sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e82f1-13c1-4765-b9be-0587bb68da97",
   "metadata": {},
   "source": [
    "- Use the `knn` model's `.fit()` method on the `X_train` data and `y_train` labels, to fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b051e7c7-64de-4c42-8326-c2303beae623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model:\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb143b-df00-4fb0-bb7d-22b899581aab",
   "metadata": {},
   "source": [
    "- Print out the `knn` model's `.score(`) on the `X_test` data and `y_test` labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cf8290e-dbb9-4afa-9027-7ee5f4a56851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model performance:\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e3f38-df5e-45c7-9577-a0242fd24ade",
   "metadata": {},
   "source": [
    "### 4.2. KNN on scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15c0df2-1b13-4c94-8ab2-4ff93d596174",
   "metadata": {},
   "source": [
    "The accuracy score on the unscaled `wine` dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous exercise, with the added step of scaling the data. Once again, the `knn` model as well as the `X` and `y` data and labels set have already been created for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1dbba-ab70-4ef7-999b-469bb33ebfd8",
   "metadata": {},
   "source": [
    "- Create the `StandardScaler()` method, stored in a variable named `ss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be2499ee-12fa-4753-90a2-f1e6fb2f0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the scaler:\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9177ead3-fe2c-48e3-97d6-4b381fd40184",
   "metadata": {},
   "source": [
    "- Apply the `ss.fit_transform()` method to the `X` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0747e67-2881-47d4-82da-ac2fc7d5c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data subset:\n",
    "X_scaled = pd.DataFrame(ss.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b31eefce-e89f-436d-b893-12d6d2f22716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the sclaed version of data into training & hold-out sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62df9fd-c910-4dcc-84be-b5d2072bae06",
   "metadata": {},
   "source": [
    "- Use the `knn` model's `.fit()` method on the `X_train` data and `y_train` labels, to fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b94a6728-77e0-43c4-81cc-cfc48f9d9a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the model:\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e848f5-e5bc-47de-a65d-a09094428268",
   "metadata": {},
   "source": [
    "- Print out the `knn` model's `.score()` on the `X_test` data and `y_test` labels to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb3dc662-d0b0-40d2-ac8c-a7a964d8de31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model performance:\n",
    "print(knn.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
