{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d2116c-e2b4-48b3-9910-43cd0bbbf7b9",
   "metadata": {},
   "source": [
    "# Chapter #1: Introduction to Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b3c629-c537-4fcd-a567-b444f697afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1895c6a-de8c-4e8c-bc40-b75d60df6010",
   "metadata": {},
   "source": [
    "## 1. What is data preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682ec09-420f-47c7-9797-950a8df78933",
   "metadata": {},
   "source": [
    "1. Preprocessing Data for Machine Learning\n",
    "> Hello! Welcome to this course on Preprocessing Data for Machine Learning. My name is Sarah Guido, and I'll be helping you learn the skills necessary for preparing data for modeling. Let's jump right in.\n",
    "\n",
    "2. What is data preprocessing?\n",
    "> Data preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling.\n",
    "\n",
    "3. Refresher on Pandas basics\n",
    "> I'm going to walk through some basics in Pandas. Most of this should be review. If it isn't, go check out other courses related to Pandas. We're going to be working with some pretty straightforward files in this course. The important line here is the `hiking.head()` line. The first thing you're going to want to do with any dataset is look at it.\n",
    "\n",
    "4. Refresher on Pandas basics\n",
    "> It's useful to be able to generate a list of the features present in your dataset. You can easily see the columns in a dataset with the columns attribute, and you can see their data type with the dtype attribute.\n",
    "\n",
    "5. Refresher on Pandas basics\n",
    "> Finally, you can quickly generate some basic stats about a dataframe like the mean, standard deviation, and quartiles using the describe method. One of the first steps you can take to preprocess your data is to remove missing data. There's a lot of ways to deal with missing data, but here we're only going to cover ways to remove either columns or rows with missing data.\n",
    "\n",
    "6. Removing missing data\n",
    "> If you wanted to drop all rows from a dataframe that contain missing values, you can do that with dropna.\n",
    "\n",
    "7. Removing missing data\n",
    "> You can drop specific rows by passing index labels to the drop function, which defaults to dropping rows.\n",
    "\n",
    "8. Removing missing data\n",
    "> Usually you'll want to focus on dropping a particular column, especially if all or most of its values are missing. You can use the drop method as well, though the parameters are different. The first parameter is the column name, in this case A. We have to specify axis=1 in order to designate that we want to drop a column.\n",
    "\n",
    "9. Removing missing data\n",
    "> What if we want to drop rows where data is missing in a particular column? We can do this with the help of boolean indexing, which is a way to filter a dataframe based on certain values. Instead of indexing a dataframe using column or row names, you can set a condition to filter your dataframe by to return a specific set of data. For example, if we wanted only rows in this dataframe where column B is equal to 7, we can filter it by selecting where column B is equal to 7.\n",
    "\n",
    "10. Removing missing data\n",
    "> First, let's take a look at how many null values we have in column B, using isnull to get null values and then using sum to output a count. So we have 2 missing values. To filter those out, we can simply use the notnull method on column B as a boolean index. This will return a dataframe where all rows have a non null value for column B.\n",
    "\n",
    "11. Let's practice!\n",
    "> Now it's your turn to get rid of missing data. Give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9f8a3-ae17-4ad7-979a-df96517a188d",
   "metadata": {},
   "source": [
    "### 1.1. Missing data - columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7d147-c767-47df-9823-560ec5ad1ada",
   "metadata": {},
   "source": [
    "We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values.\n",
    "\n",
    "How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50eddf-8aa0-4b7a-8998-fdc09c80c8c8",
   "metadata": {},
   "source": [
    "- Getting everything ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00b00f1c-893f-4bf3-b01b-520f28d277fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data & make sure that hits column is imported as a str:\n",
    "volunteer = pd.read_csv(\"./data/volunteering.csv\", dtype={'hits': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525fec49-64d4-4a8c-8bc0-8acb1af3ac8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the shape:\n",
    "volunteer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7730be16-c872-4210-8b86-8477d3b1b1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opportunity_id</th>\n",
       "      <th>content_id</th>\n",
       "      <th>vol_requests</th>\n",
       "      <th>event_time</th>\n",
       "      <th>title</th>\n",
       "      <th>hits</th>\n",
       "      <th>summary</th>\n",
       "      <th>is_priority</th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_desc</th>\n",
       "      <th>...</th>\n",
       "      <th>end_date_date</th>\n",
       "      <th>status</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Community Board</th>\n",
       "      <th>Community Council</th>\n",
       "      <th>Census Tract</th>\n",
       "      <th>BIN</th>\n",
       "      <th>BBL</th>\n",
       "      <th>NTA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4996</td>\n",
       "      <td>37004</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>Volunteers Needed For Rise Up &amp; Stay Put! Home...</td>\n",
       "      <td>737</td>\n",
       "      <td>Building on successful events last summer and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>July 30 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5008</td>\n",
       "      <td>37036</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Web designer</td>\n",
       "      <td>22</td>\n",
       "      <td>Build a website for an Afghan business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>February 01 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5016</td>\n",
       "      <td>37143</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>Urban Adventures - Ice Skating at Lasker Rink</td>\n",
       "      <td>62</td>\n",
       "      <td>Please join us and the students from Mott Hall...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>January 29 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5022</td>\n",
       "      <td>37237</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>Fight global hunger and support women farmers ...</td>\n",
       "      <td>14</td>\n",
       "      <td>The Oxfam Action Corps is a group of dedicated...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Strengthening Communities</td>\n",
       "      <td>...</td>\n",
       "      <td>March 31 2012</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5055</td>\n",
       "      <td>37425</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>Stop 'N' Swap</td>\n",
       "      <td>31</td>\n",
       "      <td>Stop 'N' Swap reduces NYC's waste by finding n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Environment</td>\n",
       "      <td>...</td>\n",
       "      <td>February 05 2011</td>\n",
       "      <td>approved</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   opportunity_id  content_id  vol_requests  event_time  \\\n",
       "0            4996       37004            50           0   \n",
       "1            5008       37036             2           0   \n",
       "2            5016       37143            20           0   \n",
       "3            5022       37237           500           0   \n",
       "4            5055       37425            15           0   \n",
       "\n",
       "                                               title hits  \\\n",
       "0  Volunteers Needed For Rise Up & Stay Put! Home...  737   \n",
       "1                                       Web designer   22   \n",
       "2      Urban Adventures - Ice Skating at Lasker Rink   62   \n",
       "3  Fight global hunger and support women farmers ...   14   \n",
       "4                                      Stop 'N' Swap   31   \n",
       "\n",
       "                                             summary is_priority  category_id  \\\n",
       "0  Building on successful events last summer and ...         NaN          NaN   \n",
       "1             Build a website for an Afghan business         NaN          1.0   \n",
       "2  Please join us and the students from Mott Hall...         NaN          1.0   \n",
       "3  The Oxfam Action Corps is a group of dedicated...         NaN          1.0   \n",
       "4  Stop 'N' Swap reduces NYC's waste by finding n...         NaN          4.0   \n",
       "\n",
       "               category_desc  ...     end_date_date    status Latitude  \\\n",
       "0                        NaN  ...      July 30 2011  approved      NaN   \n",
       "1  Strengthening Communities  ...  February 01 2011  approved      NaN   \n",
       "2  Strengthening Communities  ...   January 29 2011  approved      NaN   \n",
       "3  Strengthening Communities  ...     March 31 2012  approved      NaN   \n",
       "4                Environment  ...  February 05 2011  approved      NaN   \n",
       "\n",
       "   Longitude  Community Board Community Council  Census Tract  BIN  BBL NTA  \n",
       "0        NaN              NaN                NaN          NaN  NaN  NaN NaN  \n",
       "1        NaN              NaN                NaN          NaN  NaN  NaN NaN  \n",
       "2        NaN              NaN                NaN          NaN  NaN  NaN NaN  \n",
       "3        NaN              NaN                NaN          NaN  NaN  NaN NaN  \n",
       "4        NaN              NaN                NaN          NaN  NaN  NaN NaN  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the first 5 rows:\n",
    "volunteer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17fac7-0205-4c52-bc34-ee6b4da9bca0",
   "metadata": {},
   "source": [
    "- The dataset `volunteer` has been provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1dffc-acb7-4dae-adbc-53218e26081a",
   "metadata": {},
   "source": [
    "- Use the `dropna()` function to remove columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b1a23-3dc0-43cf-b69f-2419e453cf0c",
   "metadata": {},
   "source": [
    "- You'll have to set both the `axis=` and `thresh=` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d59181c-5cc1-4cd9-836f-e0629bb841f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(665, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping columns which have >= 3 missing values & re-exploring the shape:\n",
    "volunteer.dropna(axis=1, thresh=3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bd30b4-ae3c-43d4-858e-37ffbb93604d",
   "metadata": {},
   "source": [
    "Possible Answers:\n",
    "- 35, 24.\n",
    "- 35, 35.\n",
    "- 35, 19."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66cec0-9998-4259-8a9c-55cc397828ed",
   "metadata": {},
   "source": [
    "> 35, 24."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b9703-34b6-44ed-9adb-0e014dfd07b6",
   "metadata": {},
   "source": [
    "### 1.2. Missing data - rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0af24-85e2-4d5f-8edd-f34f436e9f22",
   "metadata": {},
   "source": [
    "Taking a look at the `volunteer` dataset again, we want to drop rows where the `category_desc` column values are missing. We're going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195979c1-c5f4-4654-b69d-9b081eed68f6",
   "metadata": {},
   "source": [
    "- Check how many values are missing in the `category_desc` column using `isnull()` and `sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71079c8e-0917-4bb5-8f13-57d01a5c6068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values in category_desc column:\n",
    "volunteer['category_desc'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d2212-06f1-48cd-917c-b4c99ebd64cc",
   "metadata": {},
   "source": [
    "- Subset the `volunteer` dataset by indexing by where `category_desc` is `notnull()`, and store in a new variable called `volunteer_subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "304a7ff5-fd4c-4120-ba03-62ee5cb5a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out missing values in category_desc column:\n",
    "volunteer_subset = volunteer[volunteer['category_desc'].notna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ba085-95aa-436a-9061-a73a4f721840",
   "metadata": {},
   "source": [
    "- Take a look at the `.shape` attribute of the new dataset, to verify it worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f039ea88-26b8-4167-804a-5c1062ccb7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(617, 35)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the shape:\n",
    "volunteer_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26998518-899b-4061-8e08-6ac632a9c08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(617, 35)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also use .dropna() with specifying the category_desc column:\n",
    "volunteer.dropna(axis=0, subset=['category_desc']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c71015-f20a-437b-b3e4-2715610c016a",
   "metadata": {},
   "source": [
    "## 2. Exploring data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eacab14-b6ec-4d28-bf88-b3706209dc64",
   "metadata": {},
   "source": [
    "1. Working With Data Types\n",
    "> Now that we've reviewed some Pandas basics, we need to start thinking about other steps we have to take in order to prepare data for modeling. One of these steps is to think about the types that are present in your dataset, because you'll likely have to transform some of these columns to other types later on. Let's take a deeper look at types as well as how to convert column types in your dataset.\n",
    "\n",
    "2. Why are types important?\n",
    "> Recall that you can check the types of a dataframe by using the dtypes attribute, like this. Pandas datatypes are similar to native python types, but there are a couple of things to be aware of. The most common types you'll be working with are object, int64, and float64 types. The object type is what Pandas uses to refer to a column that consists of string values or is of mixed types. int64 is equivalent to the Python integer type. the 64 simply refers to the allocation of memory alloted for storing the values. and float64 is equivalent to the float type. Another type you might see as you work with data in pandas is the datetime64 type (or the timedelta type). This is because you can store dates as datetime types in pandas dataframes, and even use datetimes as a special kind of index. All you need to be familiar with as we work through this course are the object, int64, and float64 types, though. Before any preprocessing can begin, you have to understand what types you're dealing with in your dataset. Sometimes, you'll start working with a dataset that has an incorrect column type: maybe a numerical column was written out into a csv as a string, and when you try to work with that column, numerical operations won't work.\n",
    "\n",
    "3. Converting column types\n",
    "> Let's take a look at how to adjust the type of a column if the type that pandas has inferred upon reading in the file is incorrect. Here we have a simple dataset with a couple of columns. if you run df.dtypes, you'll see that the type for column C is object. However, if we simply look at this dataframe, you can see that these are float values: numbers with decimal points. If we want to preprocess and model this data, we're going to have to adjust the column type.\n",
    "\n",
    "4. Converting column types\n",
    "> Changing the type of a column is very straightforward. Pandas already has a method for converting the type of the column to a new type. You can change the type using the astype method and passing in the type you want to convert it to. Make sure you're only assigning it to the column you want converted. It's also good to be as sure as you can that the column type you want to convert to is representative of the whole column. Remember that the object type can represent a column that includes both string and numeric types.\n",
    "\n",
    "5. Let's practice!\n",
    "> Now it's your turn to do some type conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6a58b-21a5-4de0-b3ba-0352ca76cf07",
   "metadata": {},
   "source": [
    "### 2.1. Exploring data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccb4e63-9cb9-40cd-acc6-619a03f52b52",
   "metadata": {},
   "source": [
    "Taking another look at the dataset comprised of volunteer information from New York City, we want to know what types we'll be working with as we start to do more preprocessing.\n",
    "\n",
    "Which data types are present in the volunteer dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf30c69-d577-4c2e-86ce-d8626338b1c7",
   "metadata": {},
   "source": [
    "- The dataset `volunteer` has been provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0309fa-64a2-440e-bb24-23eb3d92c515",
   "metadata": {},
   "source": [
    "- Use the `.dtypes` attribute to check the datatypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3392af50-bedb-4ee0-9e8d-ae2e7c5a9c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     15\n",
       "float64    13\n",
       "int64       7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the data types:\n",
    "volunteer.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949df87b-f8be-4631-9f58-188652706aca",
   "metadata": {},
   "source": [
    "Possible Answers:\n",
    "- Float and int only.\n",
    "- Int only.\n",
    "- Float, int, and object.\n",
    "- Float only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60cde2-0a34-46cc-9026-2bac1a9b34c2",
   "metadata": {},
   "source": [
    "> Float, int, and object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c94e46-c0da-4122-874e-d2f34f279c66",
   "metadata": {},
   "source": [
    "### 2.2. Converting a column type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1ae5c-4643-4b90-8416-6c61bc61d297",
   "metadata": {},
   "source": [
    "If you take a look at the `volunteer` dataset types, you'll see that the column `hits` is type `object`. But, if you actually look at the column, you'll see that it consists of integers. Let's convert that column to type `int`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7cede-44fd-449a-a033-0f747648c201",
   "metadata": {},
   "source": [
    "- Take a look at the `.head()` of the `hits` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1fb9dd8-4d31-4527-9a20-8da2a9e89f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    737\n",
       "1     22\n",
       "2     62\n",
       "3     14\n",
       "4     31\n",
       "Name: hits, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the first 5 values in the hits column:\n",
    "volunteer['hits'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e4ae0-9ce0-4406-bf8c-9a68a1da2d8f",
   "metadata": {},
   "source": [
    "- Use the `.astype` function to convert the column to type `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "761bd780-0329-4bc9-8056-2ad4d66f567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the hits column into integers:\n",
    "volunteer['hits'] = volunteer['hits'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b93cdf-aa9a-405e-9dcf-cc6da7efe9ea",
   "metadata": {},
   "source": [
    "- Take a look at the `.dtypes` of the dataset again, and notice that the column type has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d56f75df-de92-4ce9-abf7-1efcf5e8737a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opportunity_id          int64\n",
       "content_id              int64\n",
       "vol_requests            int64\n",
       "event_time              int64\n",
       "title                  object\n",
       "hits                    int32\n",
       "summary                object\n",
       "is_priority            object\n",
       "category_id           float64\n",
       "category_desc          object\n",
       "amsl                  float64\n",
       "amsl_unit             float64\n",
       "org_title              object\n",
       "org_content_id          int64\n",
       "addresses_count         int64\n",
       "locality               object\n",
       "region                 object\n",
       "postalcode            float64\n",
       "primary_loc           float64\n",
       "display_url            object\n",
       "recurrence_type        object\n",
       "hours                   int64\n",
       "created_date           object\n",
       "last_modified_date     object\n",
       "start_date_date        object\n",
       "end_date_date          object\n",
       "status                 object\n",
       "Latitude              float64\n",
       "Longitude             float64\n",
       "Community Board       float64\n",
       "Community Council     float64\n",
       "Census Tract          float64\n",
       "BIN                   float64\n",
       "BBL                   float64\n",
       "NTA                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the data types:\n",
    "volunteer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f02a2d-7861-4061-ac27-cc8c5edb6e4a",
   "metadata": {},
   "source": [
    "## 3. Class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154b05c8-41dd-483c-b050-6fccc5c7b8b3",
   "metadata": {},
   "source": [
    "1. Training and Test Sets\n",
    "> One of the most necessary steps for preprocessing, which you should be familiar with if you've taken other courses on Python and machine learning, is splitting up your data into training and test sets. We do this to avoid the issue of overfitting. If we train a model on our entire set of data, we won't have any way to test and validate our model because the model will essentially know the dataset by heart. Holding out a test set allows us to preserve some data the model hasn't seen yet.\n",
    "\n",
    "2. Splitting up your dataset\n",
    "> Just to review, this is how you split up your dataset in scikit learn using the train_test_split function. This should look familiar to you. The function shuffles up your dataset and then randomly splits it. By default, the function will split 75% of the data into the training set and 25% into the test set. In many scenarios, the default splitting parameters will work well. However, if your labels have an uneven distribution, your test and training sets might not be representative samples of your dataset and could bias the model you're trying to train. For example, if you look at the example training and test datasets on this slide, you can see that the training set has only samples labeled n, while there is a y label in the test set.\n",
    "\n",
    "3. Stratified sampling\n",
    "> A good technique for sampling more accurately when you have imbalanced classes is stratified sampling, which is a way of sampling that takes into account the distribution of classes or features in your dataset. So for example, let's say we had a dataset with 100 samples, 80 of which are class 1 and 20 of which are class 2. We want the class distribution in both our training set and our test set to reflect this, so in both our training and test sets, we'd want 80% of our sample to be class 1 and 20% to be class 2, which means we'd want 60 class 1 samples and 15 class 2 samples in our training set of 75 samples. In our test set of 25 samples, we want to have 20 class 1 samples and 5 of class 2. This is on par with the distribution of classes in the original dataset.\n",
    "\n",
    "4. Stratified sampling\n",
    "> There's a really easy way to do this in scikit learn using the train test split function. The function comes with a stratify parameter, and to stratify according to class labels, just pass in your y dataset to that parameter. So here we have our 100 labels, 80 are class1 and 20 are class 2. let's run train_test_split, and pass the y labels dataset into that stratify parameter.\n",
    "\n",
    "5. Stratified sampling\n",
    "> If we check the distribution of classes for our training and test labels, you can see the distribution of classes is in accordance with the original y class distribution.\n",
    "\n",
    "6. Let's practice!\n",
    "> Now it's your turn to do some stratified sampling!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8417fdf0-ee4d-4e5b-b6bb-10c26dc10ce3",
   "metadata": {},
   "source": [
    "### 3.1. Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd136fa7-7b43-4c3c-ab27-3bd0c7d08ea0",
   "metadata": {},
   "source": [
    "In the `volunteer` dataset, we're thinking about trying to predict the `category_desc` variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label.\n",
    "\n",
    "Which descriptions occur less than 50 times in the `volunteer` dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cef933-64c1-405e-9bd7-5723136e9064",
   "metadata": {},
   "source": [
    "- The dataset `volunteer` has been provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9a028-d803-46cc-8923-9e3c656c3124",
   "metadata": {},
   "source": [
    "- The colum you want to check is `category_desc`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa174e6-e928-471b-a333-6cf6c0549938",
   "metadata": {},
   "source": [
    "- Use the `.value_counts()` method to check variable counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a28e80-b07e-467a-8515-0aa64e645d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Strengthening Communities    307\n",
       "Helping Neighbors in Need    119\n",
       "Education                     92\n",
       "Health                        52\n",
       "Environment                   32\n",
       "Emergency Preparedness        15\n",
       "Name: category_desc, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the frequency of different values in category_desc column:\n",
    "volunteer['category_desc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220055d6-fcd6-46f6-a89d-aaa38f663860",
   "metadata": {},
   "source": [
    "Possible Answers:\n",
    "- Emergency Preparedness.\n",
    "- Health.\n",
    "- Environment.\n",
    "- 1 and 3.\n",
    "- All of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4beb0f-f4a0-4d16-aacd-670a03c0da16",
   "metadata": {},
   "source": [
    "> 1 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf1cba-d42b-4f42-b63d-7f75871c3c1f",
   "metadata": {},
   "source": [
    "### 3.2. Stratified sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53debe-f486-4aad-9c96-40656cfc5b31",
   "metadata": {},
   "source": [
    "We know that the distribution of variables in the `category_desc` column in the `volunteer` dataset is uneven. If we wanted to train a model to try to predict `category_desc`, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b73e4bd-17b8-4b5f-a31c-a4ca98465765",
   "metadata": {},
   "source": [
    "- Getting everything ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "631ca90f-a511-4d5a-8007-745246148871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows in which target column category_desc is missing:\n",
    "volunteer.dropna(axis=0, subset=['category_desc'], inplace=True)\n",
    "\n",
    "# Dropping columns which have missing values:\n",
    "volunteer.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc16f5b-66a4-4646-b5f8-956314878bdb",
   "metadata": {},
   "source": [
    "- Create a `volunteer_X` dataset with all of the columns except `category_desc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0ceb577-8240-49ff-9af2-d536e6832b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature matrix (X):\n",
    "volunteer_X = volunteer.drop(columns='category_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df9c065-e702-467d-b4f5-fa2c651c2fec",
   "metadata": {},
   "source": [
    "- Create a `volunteer_y` training labels dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bba03481-14ef-4a2d-b2d1-77dab642078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the target column (y):\n",
    "volunteer_y = volunteer['category_desc'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bb3b1e-b478-4275-be5e-6ee197e315a4",
   "metadata": {},
   "source": [
    "- Split up the `volunteer_X` dataset using scikit-learn's `train_test_split` function and passing `volunteer_y` into the `stratify=` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f34c031-0dd4-4247-a09f-37991c4c433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training & hold-out sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37ffe95-05a7-4fd6-b9eb-605f6627a4a7",
   "metadata": {},
   "source": [
    "- Take a look at the `category_desc` value counts on the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83da0ad6-ea53-4122-bf22-08cab30f26b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strengthening Communities    0.497835\n",
      "Helping Neighbors in Need    0.192641\n",
      "Education                    0.149351\n",
      "Health                       0.084416\n",
      "Environment                  0.051948\n",
      "Emergency Preparedness       0.023810\n",
      "Name: category_desc, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checking the value counts % in training set:\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a22a924e-73a4-41a9-9ab9-80df7d542d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strengthening Communities    0.496774\n",
      "Helping Neighbors in Need    0.193548\n",
      "Education                    0.148387\n",
      "Health                       0.083871\n",
      "Environment                  0.051613\n",
      "Emergency Preparedness       0.025806\n",
      "Name: category_desc, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checking the value counts % in hold-out set:\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
